---
title: "<center><h1>Project R</h1><h3>Creating a More Reproducible, Web-Based Study</h3></center>"
author: "Michael Gilbert"
date: "3/15/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

<h1><center>Introduction</center></h1>
One of the more salient barriers to performing high-quality psychological research, especially in these times of pandemic and social distancing, is reproducibility.  Although the so-called "reproducibility crisis" is surely overblown and not nearly as crisis-ish as it is claimed, the fact remains that the foundation of science is in fact broad replicability of results, the better to nail down on the underlying "universal laws" beneath.  Of course, in Psychology, one quickly learns that there are no such laws, discounting chaos theory; reality may be deterministic at a molecular level, but at the level of the whole thinking, acting human, behavior may be impacted by literally uncountable influences. These include development, language, culture, affect, prior learning and any of the hundreds of other factors you can conceive of acting on a person at a given place and time. As the complexity of the underlying situation comes to the fore, the concept of "laws" begins to feel even a little quaint and naive.  By the same token, however, we cannot as a field allow a single study on a single population - most likely a convenience sample of Psychology undergrads at the same institution the researcher works at - to dictate theory and practice going forward.  Replication and standardization must become a higher priority if we intend to truly take our work to the next level.

However, I hear the responses saying, "Replication is hard!"  I can't deny that it is true.  Replicating studies takes time, which costs money, and may require specialized materials or apparatus that the original researcher has created, and may be reticent to share for one reason or another.  The predatory, adversarial world of scientific publishing and the fear of being "scooped", of losing publishing privileges and the accompanying clout, the all important Nature Paper credential hanging upon one's wall and lauded in one's CV, does push things in undesirable directions.  Open science and access is a step in the right direction, but it will be years at best before these efforts show fruit in reducing the downsides of our unduly capitalist-lite "everyone for themselves" academic culture into one that more closely represents the cooperative spirit of scientific progress.

But, more to the point, what if replication could be made simpler?  Questions of sampling and individual differences will likely never be solved by but a single study alone, for the real answer there is to simply add more samples and more data points to the mix. How can this be achieved, in a simpler, less costly way?  By bundling all assets and methods of a study together and freely disseminating them to the world, that an identical or consciously modified version of the same exact experiment may be simply and quickly deployed on a variety of populations.  

All the better if it can be done online, as well; though the vagaries of the Internet Age have brought with them some negative aspects it has nonetheless significantly improved many people's access to institutions and researchers they might otherwise not have been able to attend or participate in research from.  This presents other challenges, of course, as remote procedures bring with them a host of other challenges, such as reduced control, equipment variation on the participants' end, and just plain not fully knowing what the participants are doing at all times in the research.  That said, these are not insurmountable problems, and may not make the data gathered using such techniques meaningless, either.  A virtual face-to-face proctoring chat on Zoom, or at least talking head to talking head, can go a long way here, as well as employing cognizant, thoughtful design when creating the experimental materials.

<h3><center>The Project</center></h3>

With all this in mind, I have set out to update an older, in fact old-fashioned research project I performed at a prior university, an investigation of the comparative distraction rates of external loud cafeteria noise, music with lyrics and music without lyrics, as measured by performance on two classic serial recall list memorization tasks and a longer-term text memory task. This project is a somewhat classically-designed experiment that aims to tighten up the comparison vectors utilized in other studies in the field. Rather than comparing disparate music types, which may vary in terms of instrumentation, timbre, beats per minute and other audiological factors, this project aims to directly compare pop music, the likes of which young people actively listen to and are likely to have on their phones and music players, and karaoke instrumental tracks of the same songs, which have the same tempo, rhythmic patterns and relative loudness changes.  The study also utilized a within-subjects repeated measures design to minimize individual differences in reaction to the auditory stimuli, whether it be the songs themselves, the sonic characteristics of the music, or the noise, as well as relative differences in ability and motivation to memorize lists and text.

Despite the care spent in the planning stages of the experiment, some of the methods and procedures employed in the study remain somewhat idiosyncratic at best, and difficult to export and verify.  For example, the participants in the original study wrote responses on paper, heard stimuli from an old pair of headphones that were dug up out of an old drawer in the PI's office, and scoring was all done by hand, first hand grading the responses and then hand entering the scores into Excel and SPSS.  Even storage of the response media became a significant issue, when an initially unnoticed coffee spill and subsequent neglect ruined one entire box of response sheets, thankfully after they had already been entered into the computer.  It is, in short, an experiment with some promising theoretical aspects, but in significant need of care and upgrades in the practical and methodological side of things.

In response to both these conundrums and the replication-related issues noted above, this experiment is now in the process of being translated from a semi-automated in-person experiment to an online one that can be accessed remotely and disseminated freely to other laboratories and interested parties over the internet.  To do this, a language known as Twine is being leveraged, a largely JavaScript based project that enables the creation of hypertext HTML pages, almost exactly like modern websites that can be created in largely the same way, styled with CSS and other familiar coding schemes, and also, crucially, support variables, functions and other aspects of code that make standard web development unsuitable for most experimental schemes.  The files created by this process are also highly portable, being literally just HTML stuffed with a number of Javascript methods, and can be run perfectly in almost any browser, though admittedly small-screen mobile applications require a certain level of responsive coding that may not be catered to in this early incarnation. 

<h1><center>Process</center></h1>

In contrast with my rosy, hopeful prose above, the process of actively converting the experiment has been somewhat troubled, dark and stormy.  Done way too late.  Questionably "done" at all.  Still, we press on.

<h3>Original Format</h3>

For historical reference, I have included the first version of the automated process utilized in this experiment.  The original vintage utilized a PowerPoint presentation that used the internal timing feature to advance and display the stimuli according to a timing function I created that corresponds to the lengths of the song stimuli I assembled in Audacity software.  This was a somewhat touchy process and it did get off-synch for a couple of early participants when it was revealed that I was somehow using an old version of the word lists for the third task, which threw off balance entirely in the end cases.  This was found and fixed eventually, but those cases were essentially useless and dropped from the final analysis.  The new, more modern version I am creating will have rigorous pre-testing to prevent issues of this sort from showing up in production runs with real participants.

<h3>Prototyping & Code Base Issues</h3>

Unfortunately, the process of creating the new version has not been entirely smooth.  The first major setback was I was forced to refactor the code to use a different library set.  Twine utilizes a variety of pre-built javascript functions to make for easier, more natural-language coding in the creation process.  Unfortunately, the one I began using is aimed more at basic use and beginners, and while it is definitely more user-friendly in a number of ways, it also lacks built-in support for text entry methods that are central to this experiment.  The early prototype I created using this library is included in the second folder, EarlyPrototype for reference.  I do find the larger font and styling to be somewhat more inherently appealing, and will likely be applying it via CSS to the more functional prototype at a later date.

The good news is that there are other libraries available for Twine that do a better job of providing access to full JavaScript functionality and web input methods.  I created a new prototype using a different library that is closer to the metal, so to speak, while still retaining a fair amount of usability.  This new process required a rather laborious search/replace method to convert the various functions it uses from one format to the other, but fortunately most of the actual functions themselves also exist in both libraries, as they are essentially built-in JavaScript nicely sealed in more user-friendly wrappers.  The new library has both standard text boxes and larger-scale web text area support, so grabbing the user input was comparatively a snap.

Once I got the refactoring out of the way, the rest of the project fell into place relatively smoothly - or, at least, to the extent that it's been completed thus far.  Due to time constraints and a somewhat ill-timed request from my advisor to actually work on current research projects instead of digitizing old ones, I have only implemented the first round of the three that the within-subjects design calls for.  

<h3>Audio Issues</h3>

Also somewhat unfortunately, the method I was hoping to use of compressing and inserting the sound files the experiment runs on has been hampered by the, ah, apparent loss of the individual components in disassembled form.  I still have original files in compiled format, where they have been coded into a single file that runs the entire length of the experiment, but the initial pieces, individual songs and perhaps most importantly the background noise samples, are not available at this exact moment*.  These files are not huge but at about 55mb they are slightly larger than most online base64 converters will accept, sadly.  This is definitely a hindrance to the project's main goal, which is to evaluate the effects of listening to these sounds while doing the cognitive tasks... but for testing purposes and the conceptual display of experiment upgrades, I hope that the present efforts will be sufficient.  

*I have them on a CD. I do not, however, have a working computer CD drive right now, as I haven't used one in years and did not install one in my current PC. I wonder if this is how people who put data on tapes and punch cards felt, several years down the line...

Although it is not integrated into the presentation in the way I would dearly have liked to, I have included one of these compiled audio files in the AudioSamples directory.  The noise sample is not included in these files as it was played back via external 360 degree speakers during the original experiment.  I have also included it in the audio folder.  If you play both tracks at once and start the ABC.mp3 when you hear a tone in the first few seconds of background noise.mp3, that will closely approximate the way I actually set them up in the original.  In the future, of course, all these sounds will be driven through the software and will not require pressing the Play button on an old iPod like the originals did. 

<h1><center>Upgrades</center></h1>

<h3><center>Efficiency & Fidelity</center></h3>
Regardless of the issues highlighted above, this new way of doing the experiment is vastly more efficient.  The effort and error potential of hand-grading the response sheets and then laboriously entering into a database has been replaced by an automated scoring method for the first two major tasks.  This code is methodologically relevant to this course, as it both replaces an inefficient by-hand process and uses both custom functions and automation code to examine the participant input, concatenate it to an array, change all responses to uppercase, compare those responses to a reference list, and compile a global score.  The code then combines all participant input collected by the experiment and saves it via a JavaScript export function to a .csv file, which is currently downloaded into the same directory the experiment file is run from.  I have included it in a code block below.

```
Scoring Participant $participant...
<<set $srArr = [$srA1,$srA2,$srA3,$srA4,$srA5,$srA6,$srA7,$srA8,$srA9,$srA10]>>
<<set $srArrCaps to ["blah"]>>
<<for _i to 0; _i < 9; _i++>><<set $srArrCaps[_i] to $srArr[_i].toUpperCase()>><</for>>
<<set $erArr = [$erA1,$erA2,$erA3,$erA4,$erA5,$erA6,$erA7,$erA8,$erA9,$erA10]>>
<<set $erArrCaps to ["B"]>>
<<for _i to 0; _i < 9; _i++>><<set $erArrCaps[_i] to $erArr[_i].toUpperCase()>><</for>>
<<set $erEvalArr = [$w1aReact,$w1aReact,$w2aReact,$w3aReact,$w4aReact,$w5aReact,$w6aReact,$w7aReact,$w8aReact,$w9aReact,$w10aReact]>>
<<set _pt to playTime()>>
<<set _ptSec to playTime('seconds')>>
<<set _ptMin to playTime('minutes')>>
<<set $ptX to ((_ptMin * 60)+ _ptSec)>>
<<set $srScore to 0>>
<<for _i to 0; _i < 9; _i++>><<print $srArrCaps[_i]+", ">><<if $srArrCaps[_i] == $SRwordListA[_i]>><<set $srScore += 1>><</if>><</for>>
Raw Serial Recall Score: <<print $srScore>>
<<set $erScore to 0>>
<<for _i to 0; _i < 9; _i++>><<print $erArrCaps[_i]+", ">><<if $erArrCaps[_i] == $ERwordListA[_i]>><<set $erScore += 1>><</if>><</for>>
Raw Episodic Recall Score: <<print $erScore>>
/*<<set _tileListOutput[_i] to ["MedicalTile"+(_i+1), $tileListShuf[_i].title, $tileListShuf[_i].valType, $tileVArr[_i], $tileTimeArr[_i] ]>><<set _tileListPhoneOutput[_i] to ["PhoneTile"+(_i+1), $tileListPhoneShuf[_i].title, $tileListPhoneShuf[_i].valType, $tilePhoneVArr[_i], $tilePhoneTimeArr[_i] ]>><</for>>*/
<<set $x = [$srArr,$srScore,$erArr,$erScore,$erEvalArr,$trA,$ptX]>>
<<set $x2 = $x.toString()>>
<<export $x2 $participant 'text'>>
```

Next on the agenda is to convert the messy and complex offline, manual scoring of the text recall portion to an automated format.  This is regrettably significantly more challenging than simply comparing the values of two arrays to one another, and may require more complex computational resources.  Understanding and scoring natural language text corpuses is a difficult task for usual computers, though I have heard good things about current machine learning and AI methods.  Of course, whether those are an appropriate venue of response here is a valid question. Similar changes might be avoided by varying the design of the experiment to utilize a different measure of slightly longer term memory that is more readily scored and understood computationally.  Of course, there is also the brute force Human Resources methods, as collaring a couple of willing RA's and asking them to score the text sections, with accompanying inter-rater reliability assessments and tiebreaker analyses by the PI would be a relatively plausible means of performing this complex scoring activity in the absence of high-end AI interpretive methods.  (Though again I've been hearing great things about GPT-3 and successor models... just have to get my hands on a high-end CUDA machine and, uh, also learn how to code those sorts of things.  I've played with the output, if nothing else...  If you haven't had an opportunity to check it out, visit www.AIDungeon.com to see how this sort of thing can play out when put to the purpose of running a text-based RPG or adventure game.)

<h3><center>Sharing & Reproducibility</center></h3>

Even in incomplete form, this new method is leaps and bounds ahead of the old-school, analogue format this experiment originally existed in.  By simple virtue of uploading it to GitHub in the first place, it is now vastly more accessible and interpretable than the original study was; now, again, even in sketchy format, more than 100% more people will have viewed, downloaded and executed the code than the original called for.  (This is entirely because the original number there was, ah, one.  But still, the math cannot be argued with.)

Once fully feature-complete and coded, online data repositories might be established on GitHub or similar sites.  Although the experiment files themselves are a couple hundred mb when collated together, as the final project is intended, this is a fairly small amount by the standards of modern computers and broadband connections, and the data output files are as sleek as they come as tiny csv files.  Once all is said and done, remote versions of the materials could be downloaded for use in labs, and even fully remote participants could download the files and complete the experiment wholly on their own machines.  This does of course raise questions of experimental control, whether participatns who are cheating by using outside memory like taking pictures or writing things down, participants who instead get on their phones instead of paying attention, and participants whose headphones are not very good quality or whose browsers are somehow incapable of running the experimental files properly, despite their general broad-base usability... but these questions are no different than the thousands of other projects similarly hampered by COVID-19 social distancing.  Concurrent Zoom chat proctoring might go a long way towards helping ensure the compliance of participants, and attention checks can be simply coded into later iterations of the software, particularly towards the end of the second and third loops, where fatigue setting in may be a substantial factor.

<H1><center>Data Comparisons</center></h1>

The final section I had planned for this project was meant to be a comparison of the old data used in my experiment with some simulations of output from the new version.  Unfortunately, due to time constraints and the unfinished nature of the whole thing here, this has not fallen into place like I hoped it would.

I have included the original SPSS data file that was analyzed for the final presentation of the original form of this research in the folder 4OldData.  It can be loaded directly into R with a little finagling using the built-in "foreign" package.

```{r}
library(foreign)
olddata = read.spss("4OldData/fullset.sav", use.value.labels = T, to.data.frame = T)
head(olddata)
```

This gives us a decent picture of what was going on in the old data set.  There are a number of demographic questions I considered pertinent to how distracting lyrical music in English might be, including a question as to whether English was the participant's first language. (I am very glad I created robust value labels in SPSS back in the day that can now be easily transferred to R factors.)  English as a first language did not have any significant predictive value of performance in this data, though I remain curious as to whether it has an effect or not.

I also noted the order of the sound conditions; in order to counterbalance against fatigue effects, participants in the original (and, once completed, in the new) version of the study will have semi-randomized song stimuli: the order of the samples themselves will remain constant, but whether the first set of tasks will have A, the noise-only condition, B, the non-lyrical Karaoke versions of the songs, or C. the full Lyrical songs is random at a per-participant level and intended to be collected in full Latin Square rotations.  Pure randomization would also solve this problem, but I am still somewhat interested in whether the presentation order has a notable interaction of any kind with any of the sound conditions.  Thus it was retained and used as a factor in the initial MANOVA analysis.  The old data did not find any effect of Order, which was nice to know.

Moving on, we see that all 84 cases were performed under the "Cafeteria Noise" condition. A planned "No Noise" between-subjects comparison condition had to be scrapped in the interest of time back in the day.  The final version of the new version will include this as a switch available to the experimenter.

Next, we see our three data conditions.  These are correct scores of each participant on the cognitive measures under the three noise conditions: M0, noise only, M1, Karaoke tracks, and M2, full Lyrical tracks.  The three tasks are coded as SRT, serial-recall task for the first more classic "memorize this list in order" task, ESRT, Episodic Serial Recall Task for the second memory task that asked participants to instead think about the positive or negative value of each word and then recall them at the end, and finally the DTRT, the Delayed Text Recall Task.  This last task was the only one that showed any major, significant differences across the three noise conditions, so in the end the laborious kludge of grading them proved worthwhile if nothing else.

Following this are a couple columns where I listed how many categories a given participant received a perfect score on and an attempt to estimate how this ceiling effect affected things.  Following that, a series of post-experiment questions I asked and ran some simple correlations on in an attempt to probe whether participants who consciously found music especially distracting actually did worse on the tasks than those who did not.  They are 9-point Likert scales.  SongsListed was a binary dummy code noting whether the participant listed any of tracks as being especially distracting as compared to the others.  (For some reason, this file does not contain their actual responses to that question, which I believe I intended to use when creating follow-up experiments and for future reference. At least I was thinking ahead, I suppose.)  Finally, a notes field which contains the "Multi" and "Other" ethnicity responses and some notes on the participants' file.  Looking back over these, I see a couple cases I probably would have dropped now, whether from misunderstanding instructions or other issues.

This file was cleaned with a fairly basic checking procedure where I went back over the original files one by one and made sure they were accurately entered.  I did kick out a couple cases where one person refused to write anything after the first round and a couple other oddities.

