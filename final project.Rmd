---
title: "<center><h1>Project R</h1><h3>Creating a More Reproducible, Web-Based Study</h3></center>"
author: "Michael Gilbert"
date: "3/15/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

<h1><center>Introduction</center></h1>
One of the more salient barriers to performing high-quality psychological research, especially in these times of pandemic and social distancing, is reproducibility.  Although the so-called "reproducibility crisis" is surely overblown and not nearly as crisis-ish as it is claimed, the fact remains that the foundation of science is in fact broad replicability of results, the better to nail down on the underlying "universal laws" beneath.  Of course, in Psychology, one quickly learns that there are no such laws, discounting chaos theory; reality may be deterministic at a molecular level, but at the level of the whole thinking, acting human, behavior may be impacted by literally uncountable influences. These include development, language, culture, affect, prior learning and any of the hundreds of other factors you can conceive of acting on a person at a given place and time. As the complexity of the underlying situation comes to the fore, the concept of "laws" begins to feel even a little quaint and naive.  By the same token, however, we cannot as a field allow a single study on a single population - most likely a convenience sample of Psychology undergrads at the same institution the researcher works at - to dictate theory and practice going forward.  Replication and standardization must become a higher priority if we intend to truly take our work to the next level.

However, I hear the responses saying, "Replication is hard!"  I can't deny that it is true.  Replicating studies takes time, which costs money, and may require specialized materials or apparatus that the original researcher has created, and may be reticent to share for one reason or another.  The predatory, adversarial world of scientific publishing and the fear of being "scooped", of losing publishing privileges and the accompanying clout, the all important Nature Paper credential hanging upon one's wall and lauded in one's CV, does push things in undesirable directions.  Open science and access is a step in the right direction, but it will be years at best before these efforts show fruit in reducing the downsides of our unduly capitalist-lite "everyone for themselves" academic culture into one that more closely represents the cooperative spirit of scientific progress.

But, more to the point, what if replication could be made simpler?  Questions of sampling and individual differences will likely never be solved by but a single study alone, for the real answer there is to simply add more samples and more data points to the mix. How can this be achieved, in a simpler, less costly way?  By bundling all assets and methods of a study together and freely disseminating them to the world, that an identical or consciously modified version of the same exact experiment may be simply and quickly deployed on a variety of populations.  All the better if it can be done online, as well; though the vagaries of the Internet Age have brought with them some negative aspects it has nonetheless significantly improved many people's access to institutions and researchers they might otherwise not have been able to attend or participate in research from.  This presents other challenges, of course, as remote procedures bring with them a host of other challenges, such as reduced control, equipment variation on the participants' end, and just plain not fully knowing what the participants are doing at all times in the research.  That said, these are not insurmountable problems, and may not make the data gathered using such techniques meaningless, either.  A virtual face-to-face proctoring chat on Zoom, or at least talking head to talking head, can go a long way here, as well as employing cognizant, thoughtful design when creating the experimental materials.

<h3><center>The Project</center></h3>

With all this in mind, I have set out to update an older, in fact old-fashioned research project I performed at a prior university, an investigation of the comparative distraction rates of external loud cafeteria noise, music with lyrics and music without lyrics, as measured by performance on two classic serial recall list memorization tasks and a longer-term text memory task. This project is a somewhat classically-designed experiment that aims to tighten up the comparison vectors utilized in other studies in the field. Rather than comparing disparate music types, which may vary in terms of instrumentation, timbre, beats per minute and other audiological factors, this project aims to directly compare pop music, the likes of which young people actively listen to and are likely to have on their phones and music players, and karaoke instrumental tracks of the same songs, which have the same tempo, rhythmic patterns and relative loudness changes.  The study also utilized a within-subjects repeated measures design to minimize individual differences in reaction to the auditory stimuli, whether it be the songs themselves, the sonic characteristics of the music, or the noise, as well as relative differences in ability and motivation to memorize lists and text.

Despite the care spent in the planning stages of the experiment, some of the methods and procedures employed in the study remain somewhat idiosyncratic at best, and difficult to export and verify.  For example, the participants in the original study wrote responses on paper, heard stimuli from an old pair of headphones that were dug up out of an old drawer in the PI's office, and scoring was all done by hand, first hand grading the responses and then hand entering the scores into Excel and SPSS.  Even storage of the response media became a significant issue, when an initially unnoticed coffee spill and subsequent neglect ruined one entire box of response sheets, thankfully after they had already been entered into the computer.  It is, in short, an experiment with some promising theoretical aspects, but in significant need of care and upgrades in the practical and methodological side of things.

In response to both these conundrums and the replication-related issues noted above, this experiment is now in the process of being translated from a semi-automated in-person experiment to an online one that can be accessed remotely and disseminated freely to other laboratories and interested parties over the internet.  To do this, a language known as Twine is being leveraged, a largely JavaScript based project that enables the creation of hypertext HTML pages, almost exactly like modern websites that can be created in largely the same way, styled with CSS and other familiar coding schemes, and also, crucially, support variables, functions and other aspects of code that make standard web development unsuitable for most experimental schemes.  The files created by this process are also highly portable, being literally just HTML stuffed with a number of Javascript methods, and can be run perfectly in almost any browser, though admittedly small-screen mobile applications require a certain level of responsive coding that may not be catered to in this early incarnation. 

<h1><center>Process</center></h1>

Troubled, dark and stormy. Done way too late.  Questionably "done" at all.

<h1><center>Upgrades</center></h1>

<h3><center>Efficiency</center></h3>
This new way of doing the experiment is vastly more efficient.  The effort and error potential of hand-grading the response sheets and then laboriously entering into a database has been replaced by 

<h3><center>Fidelity</center></h3>
This new way of doing the experiment is vastly more efficient.

<h3><center>Sharing/Reproducibility</center></h3>
This new way of doing the experiment is vastly more efficient.
