---
title: "<center><h1>Project R</h1><h3>Creating a More Reproducible, Web-Based Study</h3></center>"
author: "Michael Gilbert"
date: "3/15/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

<h1><center>Introduction</center></h1>
One of the more salient barriers to performing high-quality psychological research, especially in these times of pandemic and social distancing, is reproducibility.  Although the so-called "reproducibility crisis" is surely overblown and not nearly as crisis-ish as it is claimed, the fact remains that the foundation of science is in fact broad replicability of results, the better to nail down on the underlying "universal laws" beneath.  Of course, in Psychology, one quickly learns that there are no such laws, discounting chaos theory; reality may be deterministic at a molecular level, but at the level of the whole thinking, acting human, behavior may be impacted by literally uncountable influences. These include development, language, culture, affect, prior learning and any of the hundreds of other factors you can conceive of acting on a person at a given place and time. As the complexity of the underlying situation comes to the fore, the concept of "laws" begins to feel even a little quaint and naive.  By the same token, however, we cannot as a field allow a single study on a single population - most likely a convenience sample of Psychology undergrads at the same institution the researcher works at - to dictate theory and practice going forward.  Replication and standardization must become a higher priority if we intend to truly take our work to the next level.

However, I hear the responses saying, "Replication is hard!"  I can't deny that it is true.  Replicating studies takes time, which costs money, and may require specialized materials or apparatus that the original researcher has created, and may be reticent to share for one reason or another.  The predatory, adversarial world of scientific publishing and the fear of being "scooped", of losing publishing privileges and the accompanying clout, the all important Nature Paper credential hanging upon one's wall and lauded in one's CV, does push things in undesirable directions.  Open science and access is a step in the right direction, but it will be years at best before these efforts show fruit in reducing the downsides of our unduly capitalist-lite "everyone for themselves" academic culture into one that more closely represents the cooperative spirit of scientific progress.

But, more to the point, what if replication could be made simpler?  Questions of sampling and individual differences will likely never be solved by but a single study alone, for the real answer there is to simply add more samples and more data points to the mix. How can this be achieved, in a simpler, less costly way?  By bundling all assets and methods of a study together and freely disseminating them to the world, that an identical or consciously modified version of the same exact experiment may be simply and quickly deployed on a variety of populations.  All the better if it can be done online, as well; though the vagaries of the Internet Age have brought with them some negative aspects it has nonetheless significantly improved many people's access to institutions and researchers they might otherwise not have been able to attend or participate in research from.  This presents other challenges, of course, as remote procedures bring with them a host of other challenges, such as reduced control, equipment variation on the participants' end, and just plain not fully knowing what the participants are doing at all times in the research.  That said, these are not insurmountable problems, and may not make the data gathered using such techniques meaningless, either.  A virtual face-to-face proctoring chat on Zoom, or at least talking head to talking head, can go a long way here, as well as employing cognizant, thoughtful design when creating the experimental materials.

<h3><center>The Project</center></h3>

With all this in mind, I have set out to update an older, in fact old-fashioned research project I performed at a prior university, an investigation of the comparative distraction rates of external loud cafeteria noise, music with lyrics and music without lyrics, as measured by performance on two classic serial recall list memorization tasks and a longer-term text memory task. This project is a somewhat classically-designed experiment that aims to tighten up the comparison vectors utilized in other studies in the field. Rather than comparing disparate music types, which may vary in terms of instrumentation, timbre, beats per minute and other audiological factors, this project aims to directly compare pop music, the likes of which young people actively listen to and are likely to have on their phones and music players, and karaoke instrumental tracks of the same songs, which have the same tempo, rhythmic patterns and relative loudness changes.  The study also utilized a within-subjects repeated measures design to minimize individual differences in reaction to the auditory stimuli, whether it be the songs themselves, the sonic characteristics of the music, or the noise, as well as relative differences in ability and motivation to memorize lists and text.

Despite the care spent in the planning stages of the experiment, some of the methods and procedures employed in the study remain somewhat idiosyncratic at best, and difficult to export and verify.  For example, the participants in the original study wrote responses on paper, heard stimuli from an old pair of headphones that were dug up out of an old drawer in the PI's office, and scoring was all done by hand, first hand grading the responses and then hand entering the scores into Excel and SPSS.  Even storage of the response media became a significant issue, when an initially unnoticed coffee spill and subsequent neglect ruined one entire box of response sheets, thankfully after they had already been entered into the computer.  It is, in short, an experiment with some promising theoretical aspects, but in significant need of care and upgrades in the practical and methodological side of things.

In response to both these conundrums and the replication-related issues noted above, this experiment is now in the process of being translated from a semi-automated in-person experiment to an online one that can be accessed remotely and disseminated freely to other laboratories and interested parties over the internet.  To do this, a language known as Twine is being leveraged, a largely JavaScript based project that enables the creation of hypertext HTML pages, almost exactly like modern websites that can be created in largely the same way, styled with CSS and other familiar coding schemes, and also, crucially, support variables, functions and other aspects of code that make standard web development unsuitable for most experimental schemes.  The files created by this process are also highly portable, being literally just HTML stuffed with a number of Javascript methods, and can be run perfectly in almost any browser, though admittedly small-screen mobile applications require a certain level of responsive coding that may not be catered to in this early incarnation. 

<h1><center>Process</center></h1>

In contrast with my rosy, hopeful prose above, the process of actively converting the experiment has been somewhat troubled, dark and stormy.  Done way too late.  Questionably "done" at all.  Still, we press on.

The first major setback was I was forced to refactor the code to use a different library set.  Twine utilizes a variety of pre-built javascript functions to make for easier, more natural-language coding in the creation process.  Unfortunately, the one I began using is aimed more at basic use and beginners, and while it is definitely more user-friendly in a number of ways, it also lacks built-in support for text entry methods that are central to this experiment.  The early prototype I created using this library is included in the second folder, EarlyPrototype for reference.  I do find the larger font and styling to be somewhat more inherently appealing, and will likely be applying it via CSS to the more functional prototype at a later date.

The good news is that there are other libraries available for Twine that do a better job of providing access to full JavaScript functionality and web input methods.  I created a new prototype using a different library that is closer to the metal, so to speak, while still retaining a fair amount of usability.  This new process required a rather laborious search/replace method to convert the various functions it uses from one format to the other, but fortunately most of the actual functions themselves also exist, as they are essentially built-in JavaScript nicely sealed in more user-friendly wrappers.  The new library has both standard text boxes and larger-scale web text area support, so grabbing the user input was comparatively a snap.

Once I got the refactoring out of the way, the rest of the project fell into place relatively smoothly - or, at least, to the extent that it's been completed thus far.  Due to time constraints and a somewhat ill-timed request from my advisor to actually work on current research projects instead of digitizing old ones, I have only completed the first round of the three that the within-subjects design calls for.  

Also somewhat unfortunately, the method I was hoping to use of compressing and inserting the sound files the experiment runs on has been hampered by the, ah, apparent loss of the individual components in disassembled form.  I still have original files in compiled format, where they have been coded into a single file that runs the entire length of the experiment, but the initial pieces, individual songs and perhaps most importantly the background noise samples, are not available at this exact moment*.  These files are not huge but at about 55mb they are slightly larger than most online base64 converters will accept, sadly.  This is definitely a hindrance to the project's main goal, which is to evaluate the effects of listening to these sounds while doing the cognitive tasks... but for testing purposes and the conceptual display of experiment upgrades, I hope that the present efforts will be sufficient.

*I have them on a CD. I do not, however, have a working computer CD drive right now, as I haven't used one in years and did not install one in my current PC. I wonder if this is how people who put data on tapes and punch cards felt, several years down the line...

<h1><center>Upgrades</center></h1>

<h3><center>Efficiency</center></h3>
Regardless of the issues highlighted above, this new way of doing the experiment is vastly more efficient.  The effort and error potential of hand-grading the response sheets and then laboriously entering into a database has been replaced by 

<h3><center>Fidelity</center></h3>
This new way of doing the experiment is vastly more efficient.

<h3><center>Sharing/Reproducibility</center></h3>
This new way of doing the experiment is vastly more efficient.
